{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from swiplserver import PrologMQI, PrologThread\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target:\n",
    "    def __init__(self,pr,ab):\n",
    "        self.present = pr\n",
    "        self.absent = ab\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"present: {self.present} absent: {self.absent}\"    \n",
    "        \n",
    "targets = []\n",
    "interesting_genes = [\"tbet\",\"gata3\",\"foxp3\",\"rorgt\"]\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "for x in powerset(interesting_genes):\n",
    "    if (len(x)>0):\n",
    "        targets.append(Target([k for k in x],[k for k in interesting_genes if k not in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks whether a node is in an attractor\n",
    "def check_node(node):\n",
    "    cycle = list(nx.find_cycle(G,node))\n",
    "    tmp = map(lambda x : x[0]==node or x[1]==node, cycle) \n",
    "    return reduce(lambda b1, b2: b1 or b2, tmp)\n",
    "\n",
    "# compute the list of genes that are present in the attractor reachable form \"node\"\n",
    "def compute_attractor(node):\n",
    "    cycle = list(nx.find_cycle(G,node))\n",
    "    tmp1 = map(lambda x: x[0].split(';') + x[1].split(';'), cycle)\n",
    "    tmp2 = reduce(lambda x, y: x+y,tmp1)\n",
    "    res = list(dict.fromkeys(tmp2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_states (d):\n",
    "    tmp = 0\n",
    "    for v in d.values():\n",
    "        tmp += len(v)\n",
    "    return tmp\n",
    "\n",
    "# finds computations (LTS traces) that lead to the \"target\"\n",
    "def target_computations(target):\n",
    "    all_nodes = list(G.nodes)\n",
    "    filtered = [k for k in all_nodes if check_node(k)] # filters out intermediate nodes (not in attractor)\n",
    "    attractors_map = dict()\n",
    "    for f in filtered:\n",
    "        attractors_map[f] = compute_attractor(f) # creates a map \"state -> attractor\"\n",
    "    for s in target.present:\n",
    "        filtered = [k for k in filtered if s in attractors_map[k]]   # filters out states that do not contain s (present in the target)\n",
    "    for s in target.absent:\n",
    "        filtered = [k for k in filtered if s not in attractors_map[k]] # filters out state that contain s (absent in the target)\n",
    "\n",
    "    # filters out states in target attractors, but that do not contain any gene in target.present\n",
    "    # (slicing would give an empty result on these states)\n",
    "    filtered2 = [k for k in filtered if len(list(set(k.split(';')) & set(target.present)))>0]\n",
    "\n",
    "    # cleans the attractors_map from states not in target (this step is not really necessary...)\n",
    "    keys_to_delete = list() \n",
    "    for k in attractors_map.keys():\n",
    "        if k not in filtered: keys_to_delete.append(k) \n",
    "    for k in keys_to_delete:\n",
    "        del attractors_map[k] \n",
    "    \n",
    "    contexts = list()   # list of the contexts that lead to the target\n",
    "    contexts_dict = {}  # for each context, lists the states in the corresponding attractor\n",
    "    filtered_splitted = [f.split(';') for f in filtered2]\n",
    "    for f in filtered_splitted:\n",
    "        prefix = f[0:9] # the first 9 elements in the state are the context\n",
    "        pure_state = f[9:] # the others are the actual state\n",
    "        if (not prefix in contexts):\n",
    "            contexts.append(prefix)\n",
    "            contexts_dict[','.join(prefix)] = [','.join(pure_state)]\n",
    "        else:\n",
    "            contexts_dict[','.join(prefix)].append(','.join(pure_state))\n",
    "    print(\"TARGET --> \" + str(target))\n",
    "    print(\"found \" + str(len(contexts)) + \" contexts that lead to the target\")\n",
    "    print(\"found \" + str(count_states(contexts_dict) + (len(filtered)-len(filtered2))) + \" states in reachable attractors\")\n",
    "    print(\"of which \" + str(count_states(contexts_dict)) + \" with genes in the target\")\n",
    "    print()\n",
    "    return contexts, contexts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_tot = len(targets)\n",
    "target_count = 0\n",
    "print(\"TO BE ANALYZED: \" + str(target_tot) + \" target\")\n",
    "print()\n",
    "\n",
    "outfile = open(\"out.txt\",\"w\")\n",
    "for target in targets:\n",
    "    target_count = target_count+1\n",
    "    print(\"TARGET COUNT: \" + str(target_count) + \"/\" + str(target_tot))\n",
    "    contexts, contexts_dict = target_computations(target)\n",
    "\n",
    "    prolog_target = ','.join(target.present)\n",
    "\n",
    "    cont = 1\n",
    "    tot = str(count_states(contexts_dict))\n",
    "    union_set = set()\n",
    "    intersection_set = set()\n",
    "    first_time = True\n",
    "    for ctx in contexts:\n",
    "        prolog_context = ','.join(ctx)\n",
    "        prolog_target_states = contexts_dict[','.join(ctx)]\n",
    "        for i,state in enumerate(prolog_target_states):\n",
    "            print(\"TEST CASE: \" + str(cont) + \"/\" + str(tot))\n",
    "            cont=cont+1\n",
    "            print(\"CONTEXT: \" + prolog_context + \"      ATTRACTOR STATE: \" + str(i+1) + \"/\" + str(len(prolog_target_states)))\n",
    "            print(\"STATE: \" + state)\n",
    "            param_file = open(\"Bioresolve_depthfirst/params.pl\",'w')\n",
    "            param_file.write(\"myenvironment('[ x1 = ({tgfb}.x11 + {}.x0),\\n x2 = ({il23}.x21 + {}.x0),\\n x3 = ({il12}.x31 + {}.x0),\\n x4 = ({il18}.x41 + {}.x0),\\n x5 = ({il4e}.x51 + {}.x0),\\n x6 = ({il27}.x61 + {}.x0),\\n x7 = ({il6e}.x71 + {}.x0),\\n x8 = ({ifnge}.x81 + {}.x0),\\n x9 = ({tcr}.x91 + {}.x0),\\n x11 = {tgfb}.x11,\\n x21 = {il23}.x21,\\n x31 = {il12}.x31,\\n x41 = {il18}.x41,\\n x51 = {il4e}.x51,\\n x61 = {il27}.x61,\\n x71 = {il6e}.x71,\\n x81 = {ifnge}.x81,\\n x91 = {tcr}.x91,\\n x0 = {}.x0\\n]').\\n \\nmyentities([]).\\n \\n\")\n",
    "            param_file.write('mycontext(\"[' + prolog_context + ']\").\\n\\n')\n",
    "            param_file.write('mymonitor(\"[ m0 ]\").\\n\\n')\n",
    "            param_file.write('mymondef(\"[ m0 = ([{' + state + '} inW].no({' + prolog_target +'}) + [-({' + state + '} inW)].m0) ]\").\\n')\n",
    "            param_file.flush()\n",
    "            param_file.close()\n",
    "            print()\n",
    "            with PrologMQI() as mqi:\n",
    "                with mqi.create_thread() as prolog_thread:\n",
    "                    prolog_thread.query('[\"BioResolve_depthfirst/filterBioResolve.pl\"]')\n",
    "                    result = prolog_thread.query('main_do(slice,S).')\n",
    "                    tmp_set = set(chain.from_iterable(result[0]['S'])) \n",
    "                    union_set.update(tmp_set)\n",
    "                    if (first_time):\n",
    "                        intersection_set = tmp_set\n",
    "                        first_time = False\n",
    "                    else:\n",
    "                        intersection_set = intersection_set.intersection(tmp_set)\n",
    "    for f in glob.glob(\"tmp-slice*.txt\"):\n",
    "        os.remove(f)\n",
    "        \n",
    "    print()\n",
    "    union_set=sorted(union_set)\n",
    "    intersection_set=sorted(intersection_set)\n",
    "    print(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\")\n",
    "    outfile.write(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\\n\")\n",
    "    print(\"UNION: \" + str(union_set))\n",
    "    outfile.write(\"         UNION: \" + str(union_set)+\"\\n\")\n",
    "    print(\"INTERSECTION: \" + str(union_set))\n",
    "    outfile.write(\"         INTERSECTION: \" + str(intersection_set)+\"\\n\\n\")\n",
    "    print()\n",
    "    outfile.flush()\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    contexts, contexts_dict = target_computations(target)\n",
    "    if (len(contexts)>0):\n",
    "        df = pd.DataFrame(contexts).sort_values(by=[8,7,6,5,4,3,2,1,0],ignore_index=True)\n",
    "        print(\"CONTEXTS THAT LEAD TO THE TARGET:\")\n",
    "        print(df)\n",
    "        print()\n",
    "        df.to_csv(\"contexts_to_\" + str(target.present) + \".csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
