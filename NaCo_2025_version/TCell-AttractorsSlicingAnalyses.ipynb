{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T Cell Attractors and Slicing Analysis\n",
    "Tested on Python 3.11.4, networkx 3.1, pandas 1.5.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from swiplserver import PrologMQI, PrologThread\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target:\n",
    "    def __init__(self,pr,ab):\n",
    "        self.present = pr\n",
    "        self.absent = ab\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"present: {self.present} absent: {self.absent}\"    \n",
    "        \n",
    "targets = []\n",
    "interesting_genes = [\"tbet\",\"gata3\",\"foxp3\",\"rorgt\"]\n",
    "#interesting_genes = [\"gata3\"]\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "for x in powerset(interesting_genes):\n",
    "    if (len(x)>0):\n",
    "        targets.append(Target([k for k in x],[k for k in interesting_genes if k not in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks whether a node is in an attractor\n",
    "def check_node(node):\n",
    "    cycle = list(nx.find_cycle(G,node))\n",
    "    tmp = map(lambda x : x[0]==node or x[1]==node, cycle) \n",
    "    return reduce(lambda b1, b2: b1 or b2, tmp)\n",
    "\n",
    "# compute the list of genes that are present in the attractor reachable form \"node\"\n",
    "def compute_attractor(node):\n",
    "    cycle = list(nx.find_cycle(G,node))\n",
    "    tmp1 = map(lambda x: x[0].split(';') + x[1].split(';'), cycle)\n",
    "    tmp2 = reduce(lambda x, y: x+y,tmp1)\n",
    "    res = list(dict.fromkeys(tmp2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_states (d):\n",
    "    tmp = 0\n",
    "    for v in d.values():\n",
    "        tmp += len(v)\n",
    "    return tmp\n",
    "\n",
    "# finds computations (LTS traces) that lead to the \"target\"\n",
    "def target_computations(target):\n",
    "    all_nodes = list(G.nodes)\n",
    "    filtered = [k for k in all_nodes if check_node(k)] # filters out intermediate nodes (not in attractor)\n",
    "    attractors_map = dict()\n",
    "    for f in filtered:\n",
    "        attractors_map[f] = compute_attractor(f) # creates a map \"state -> attractor\"\n",
    "    for s in target.present:\n",
    "        filtered = [k for k in filtered if s in attractors_map[k]]   # filters out states that do not contain s (present in the target)\n",
    "    for s in target.absent:\n",
    "        filtered = [k for k in filtered if s not in attractors_map[k]] # filters out state that contain s (absent in the target)\n",
    "\n",
    "    # filters out states in target attractors, but that do not contain any gene in target.present\n",
    "    # (slicing would give an empty result on these states)\n",
    "    filtered2 = [k for k in filtered if len(list(set(k.split(';')) & set(target.present)))>0]\n",
    "\n",
    "    # cleans the attractors_map from states not in target (this step is not really necessary...)\n",
    "    keys_to_delete = list() \n",
    "    for k in attractors_map.keys():\n",
    "        if k not in filtered: keys_to_delete.append(k) \n",
    "    for k in keys_to_delete:\n",
    "        del attractors_map[k] \n",
    "    \n",
    "    contexts = list()   # list of the contexts that lead to the target\n",
    "    contexts_dict = {}  # for each context, lists the states in the corresponding attractor\n",
    "    filtered_splitted = [f.split(';') for f in filtered2]\n",
    "    for f in filtered_splitted:\n",
    "        prefix = f[0:10] # the first 10 elements in the state are the context [UPDATE 10!!]\n",
    "        pure_state = f[10:] # the others are the actual state\n",
    "        if (not prefix in contexts):\n",
    "            contexts.append(prefix)\n",
    "            contexts_dict[','.join(prefix)] = [','.join(pure_state)]\n",
    "        else:\n",
    "            contexts_dict[','.join(prefix)].append(','.join(pure_state))\n",
    "    print(\"TARGET --> \" + str(target))\n",
    "    print(\"found \" + str(len(contexts)) + \" contexts that lead to the target\")\n",
    "    print(\"found \" + str(count_states(contexts_dict) + (len(filtered)-len(filtered2))) + \" states in reachable attractors\")\n",
    "    print(\"of which \" + str(count_states(contexts_dict)) + \" with genes in the target\")\n",
    "    print()\n",
    "    return contexts, contexts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Creare un DataFrame globale per accumulare le righe\n",
    "table_data_df = pd.DataFrame()\n",
    "\n",
    "def generate_table(target_desc, intersection_pos_set, union_pos_set, intersection_neg_set, union_neg_set):\n",
    "    global table_data_df\n",
    "        \n",
    "    # Ottenere tutte le stringhe uniche\n",
    "    all_strings = sorted(set(table_data_df.columns.get_level_values(0)).union(union_pos_set, union_neg_set))\n",
    "    \n",
    "    # Creare l'intestazione delle colonne alternando le sottocolonne POS e NEG\n",
    "    columns = [(s, cat) for s in all_strings for cat in ('POS', 'NEG')]\n",
    "    \n",
    "    # Assicurarsi che il DataFrame abbia tutte le colonne necessarie\n",
    "    if table_data_df.empty:\n",
    "        table_data_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(columns))\n",
    "    else:\n",
    "        missing_columns = set(columns) - set(table_data_df.columns)\n",
    "        for col in missing_columns:\n",
    "            table_data_df[col] = 0\n",
    "        table_data_df = table_data_df[columns]  # Riordina le colonne\n",
    "    \n",
    "    # Creare una nuova riga\n",
    "    new_row = {col: 0 for col in table_data_df.columns}  # Inizializza tutte le colonne a 0\n",
    "    for s in all_strings:\n",
    "        if s in intersection_pos_set:\n",
    "            new_row[(s, 'POS')] = 2\n",
    "        elif s in union_pos_set:\n",
    "            new_row[(s, 'POS')] = 1\n",
    "        if s in intersection_neg_set:\n",
    "            new_row[(s, 'NEG')] = 2\n",
    "        elif s in union_neg_set:\n",
    "            new_row[(s, 'NEG')] = 1\n",
    "    \n",
    "    # Aggiungere la nuova riga al DataFrame e riempire eventuali NaN con 0\n",
    "    table_data_df = pd.concat([table_data_df, pd.DataFrame([new_row], index=[target_desc])]).fillna(0)\n",
    "    \n",
    "    # Funzione per applicare lo stile\n",
    "    def highlight_cells(val):\n",
    "        if val == 2:\n",
    "            return 'background-color: black; color: white'\n",
    "        elif val == 1:\n",
    "            return 'background-color: gray; color: white'\n",
    "        return ''\n",
    "    \n",
    "    # Applicare lo stile\n",
    "    styled_df = table_data_df.style.applymap(highlight_cells)\n",
    "    \n",
    "    # Mostrare la tabella\n",
    "    display(styled_df)\n",
    "\n",
    "def save_table_to_file(filename):\n",
    "    \"\"\"\n",
    "    Salva il DataFrame in un file CSV.\n",
    "    \"\"\"\n",
    "    global table_data_df\n",
    "    table_data_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def slicing_analysis(version):\n",
    "    target_tot = len(targets)\n",
    "    target_count = 0\n",
    "    print(\"TO BE ANALYZED: \" + str(target_tot) + \" target\")\n",
    "    print()\n",
    "\n",
    "    outfile = open(\"out.txt\",\"w\")\n",
    "    for target in targets:\n",
    "        target_count = target_count+1\n",
    "        print(\"TARGET COUNT: \" + str(target_count) + \"/\" + str(target_tot))\n",
    "        contexts, contexts_dict = target_computations(target)\n",
    "\n",
    "        prolog_target = ','.join(target.present)\n",
    "\n",
    "        cont = 1\n",
    "        tot = str(count_states(contexts_dict))\n",
    "        union_set = set()\n",
    "        intersection_set = set()\n",
    "        first_time = True\n",
    "        for ctx in contexts:\n",
    "            prolog_context = ','.join(ctx)\n",
    "            prolog_target_states = contexts_dict[','.join(ctx)]\n",
    "            for i,state in enumerate(prolog_target_states):\n",
    "                print(\"TEST CASE: \" + str(cont) + \"/\" + str(tot))\n",
    "                cont=cont+1\n",
    "                print(\"CONTEXT: \" + prolog_context + \"      ATTRACTOR STATE: \" + str(i+1) + \"/\" + str(len(prolog_target_states)))\n",
    "                print(\"STATE: \" + state)\n",
    "            \n",
    "                param_file = open(\"BioResolve_positive/params.pl\",'w')\n",
    "                if (version==\"original\"):\n",
    "                    param_file.write(\"myenvironment('[ x1 = ({tgfb}.x11 + {}.x0),\\n x2 = ({il23}.x21 + {}.x0),\\n x3 = ({il12}.x31 + {}.x0),\\n x4 = ({il18}.x41 + {}.x0),\\n x5 = ({il4e}.x51 + {}.x0),\\n x6 = ({il27}.x61 + {}.x0),\\n x7 = ({il6e}.x71 + {}.x0),\\n x8 = ({ifnge}.x81 + {}.x0),\\n x9 = ({tcr}.x91 + {}.x0),\\n x11 = {tgfb}.x11,\\n x21 = {il23}.x21,\\n x31 = {il12}.x31,\\n x41 = {il18}.x41,\\n x51 = {il4e}.x51,\\n x61 = {il27}.x61,\\n x71 = {il6e}.x71,\\n x81 = {ifnge}.x81,\\n x91 = {tcr}.x91,\\n x0 = {}.x0\\n]').\\n \\nmyentities([]).\\n \\n\")\n",
    "                elif (version==\"positive\"):\n",
    "                    param_file.write(\"myenvironment('[ x1 = ({tgfb}.x11 + {neg_tgfb}.x12),\\n x2 = ({il23}.x21 + {neg_il23}.x22),\\n x3 = ({il12}.x31 + {neg_il12}.x32),\\n x4 = ({il18}.x41 + {neg_il18}.x42),\\n x5 = ({il4e}.x51 + {neg_il4e}.x52),\\n x6 = ({il27}.x61 + {neg_il27}.x62),\\n x7 = ({il6e}.x71 + {neg_il6e}.x72),\\n x8 = ({ifnge}.x81 + {neg_ifnge}.x82),\\n x9 = ({tcr}.x91 + {neg_tcr}.x92),\\n x11 = {tgfb}.x11,\\n x21 = {il23}.x21,\\n x31 = {il12}.x31,\\n x41 = {il18}.x41,\\n x51 = {il4e}.x51,\\n x61 = {il27}.x61,\\n x71 = {il6e}.x71,\\n x81 = {ifnge}.x81,\\n x91 = {tcr}.x91,\\n x12 = {neg_tgfb}.x12,\\n x22 = {neg_il23}.x22,\\n x32 = {neg_il12}.x32,\\n x42 = {neg_il18}.x42,\\n x52 = {neg_il4e}.x52,\\n x62 = {neg_il27}.x62,\\n x72 = {neg_il6e}.x72,\\n x82 = {neg_ifnge}.x82,\\n x92 = {neg_tcr}.x92,\\n x0 = {}.x0\\n]').\\n \\n\") \n",
    "                    param_file.write(\"myentities([neg_foxp3,neg_gata3,neg_ifng,neg_ifngr,neg_il12r,neg_il17,neg_il18r,neg_il2,neg_il21,neg_il21r,neg_il23r,neg_il2r,neg_il4,neg_il4r,neg_il6,neg_il6r,neg_irak,neg_jak1,neg_nfat,neg_nfkb,neg_rorgt,neg_socs1,neg_stat1,neg_stat3,neg_stat4,neg_stat5,neg_stat6,neg_tbet,neg_tgfbr]).\\n \\n\")\n",
    "                else:\n",
    "                    print(\"Error\")\n",
    "                    return\n",
    "                param_file.write('mycontext(\"[' + prolog_context + ']\").\\n\\n')  # x0,\n",
    "                param_file.write('mytarget([' + state + ']).\\n')\n",
    "                param_file.write('mypos([' + prolog_target + ']).\\n')\n",
    "                param_file.write('myneg([]).')\n",
    "                param_file.flush()\n",
    "                param_file.close()\n",
    "                print()\n",
    "                with PrologMQI() as mqi:\n",
    "                    with mqi.create_thread() as prolog_thread:\n",
    "                        prolog_thread.query('[\"BioResolve_positive/BioReSolvePositive.pl\"]')\n",
    "                        result = prolog_thread.query('main_do(ordslice,EKs,ListReactNumbR,ListEpos,ListCS).')\n",
    "                        tmp_set = set(chain.from_iterable(result[0]['ListEpos'])) \n",
    "                        union_set.update(tmp_set)\n",
    "                        if (first_time):\n",
    "                            intersection_set = tmp_set\n",
    "                            first_time = False\n",
    "                        else:\n",
    "                            intersection_set = intersection_set.intersection(tmp_set)\n",
    "        for f in glob.glob(\"tmp-slice*.txt\"):\n",
    "            os.remove(f)\n",
    "        for f in glob.glob(\"tmp-legenda*.txt\"):\n",
    "            os.remove(f)\n",
    "        for f in glob.glob(\"tmp-slicingrun*.txt\"):\n",
    "            os.remove(f)\n",
    "        \n",
    "        print()\n",
    "        union_set=sorted(union_set)\n",
    "        intersection_set=sorted(intersection_set)\n",
    "        print(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\")\n",
    "        outfile.write(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\\n\")\n",
    "        print(\"UNION: \" + str(union_set))\n",
    "        outfile.write(\"         UNION: \" + str(union_set)+\"\\n\")\n",
    "        print(\"INTERSECTION: \" + str(intersection_set))\n",
    "        outfile.write(\"         INTERSECTION: \" + str(intersection_set)+\"\\n\\n\")\n",
    "        print()\n",
    "        outfile.flush()\n",
    "    \n",
    "        intersection_neg_set = {s[4:] for s in intersection_set if s.startswith(\"neg\")}\n",
    "        intersection_pos_set = {s for s in intersection_set if not s.startswith(\"neg\")}\n",
    "\n",
    "        union_neg_set = {s[4:] for s in union_set if s.startswith(\"neg\")}\n",
    "        union_pos_set = {s for s in union_set if not s.startswith(\"neg\")}\n",
    "    \n",
    "        generate_table(str(target.present),intersection_pos_set, union_pos_set, intersection_neg_set, union_neg_set)\n",
    "\n",
    "    outfile.close()\n",
    "    save_table_to_file(\"table_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dynamic_negative_slicing_analysis(version):\n",
    "    target_tot = len(targets)\n",
    "    target_count = 0\n",
    "    print(\"TO BE ANALYZED: \" + str(target_tot) + \" target\")\n",
    "    print()\n",
    "\n",
    "    outfile = open(\"out.txt\",\"w\")\n",
    "    for target in targets:\n",
    "        target_count = target_count+1\n",
    "        print(\"TARGET COUNT: \" + str(target_count) + \"/\" + str(target_tot))\n",
    "        contexts, contexts_dict = target_computations(target)\n",
    "\n",
    "        prolog_target = ','.join(target.present)\n",
    "\n",
    "        cont = 1\n",
    "        tot = str(count_states(contexts_dict))\n",
    "        union_pos_set = set()\n",
    "        intersection_pos_set = set()\n",
    "        union_neg_set = set()\n",
    "        intersection_neg_set = set()\n",
    "        first_time = True\n",
    "        for ctx in contexts:\n",
    "            prolog_context = ','.join(ctx)\n",
    "            prolog_target_states = contexts_dict[','.join(ctx)]\n",
    "            for i,state in enumerate(prolog_target_states):\n",
    "                print(\"TEST CASE: \" + str(cont) + \"/\" + str(tot))\n",
    "                cont=cont+1\n",
    "                print(\"CONTEXT: \" + prolog_context + \"      ATTRACTOR STATE: \" + str(i+1) + \"/\" + str(len(prolog_target_states)))\n",
    "                print(\"STATE: \" + state)\n",
    "                param_file = open(\"BioResolve_positive/params.pl\",'w')\n",
    "                if (version==\"original\"):\n",
    "                    param_file.write(\"myenvironment('[ x1 = ({tgfb}.x11 + {}.x0),\\n x2 = ({il23}.x21 + {}.x0),\\n x3 = ({il12}.x31 + {}.x0),\\n x4 = ({il18}.x41 + {}.x0),\\n x5 = ({il4e}.x51 + {}.x0),\\n x6 = ({il27}.x61 + {}.x0),\\n x7 = ({il6e}.x71 + {}.x0),\\n x8 = ({ifnge}.x81 + {}.x0),\\n x9 = ({tcr}.x91 + {}.x0),\\n x11 = {tgfb}.x11,\\n x21 = {il23}.x21,\\n x31 = {il12}.x31,\\n x41 = {il18}.x41,\\n x51 = {il4e}.x51,\\n x61 = {il27}.x61,\\n x71 = {il6e}.x71,\\n x81 = {ifnge}.x81,\\n x91 = {tcr}.x91,\\n x0 = {}.x0\\n]').\\n \\nmyentities([]).\\n \\n\")\n",
    "                elif (version==\"positive\"):\n",
    "                    param_file.write(\"myenvironment('[ x1 = ({tgfb}.x11 + {neg_tgfb}.x12),\\n x2 = ({il23}.x21 + {neg_il23}.x22),\\n x3 = ({il12}.x31 + {neg_il12}.x32),\\n x4 = ({il18}.x41 + {neg_il18}.x42),\\n x5 = ({il4e}.x51 + {neg_il4e}.x52),\\n x6 = ({il27}.x61 + {neg_il27}.x62),\\n x7 = ({il6e}.x71 + {neg_il6e}.x72),\\n x8 = ({ifnge}.x81 + {neg_ifnge}.x82),\\n x9 = ({tcr}.x91 + {neg_tcr}.x92),\\n x11 = {tgfb}.x11,\\n x21 = {il23}.x21,\\n x31 = {il12}.x31,\\n x41 = {il18}.x41,\\n x51 = {il4e}.x51,\\n x61 = {il27}.x61,\\n x71 = {il6e}.x71,\\n x81 = {ifnge}.x81,\\n x91 = {tcr}.x91,\\n x12 = {neg_tgfb}.x12,\\n x22 = {neg_il23}.x22,\\n x32 = {neg_il12}.x32,\\n x42 = {neg_il18}.x42,\\n x52 = {neg_il4e}.x52,\\n x62 = {neg_il27}.x62,\\n x72 = {neg_il6e}.x72,\\n x82 = {neg_ifnge}.x82,\\n x92 = {neg_tcr}.x92,\\n x0 = {}.x0\\n]').\\n \\n\") \n",
    "                    param_file.write(\"myentities([neg_foxp3,neg_gata3,neg_ifng,neg_ifngr,neg_il12r,neg_il17,neg_il18r,neg_il2,neg_il21,neg_il21r,neg_il23r,neg_il2r,neg_il4,neg_il4r,neg_il6,neg_il6r,neg_irak,neg_jak1,neg_nfat,neg_nfkb,neg_rorgt,neg_socs1,neg_stat1,neg_stat3,neg_stat4,neg_stat5,neg_stat6,neg_tbet,neg_tgfbr]).\\n \\n\")\n",
    "                else:\n",
    "                    print(\"Error\")\n",
    "                    return\n",
    "                param_file.write('mycontext(\"[' + prolog_context + ']\").\\n\\n')  # x0,\n",
    "                param_file.write('mytarget([' + state + ']).\\n')\n",
    "                param_file.write('mypos([' + prolog_target + ']).\\n')\n",
    "                param_file.write('myneg([]).')\n",
    "                param_file.flush()\n",
    "                param_file.close()\n",
    "                print()\n",
    "                with PrologMQI() as mqi:\n",
    "                    with mqi.create_thread() as prolog_thread:\n",
    "                        prolog_thread.query('[\"BioResolve_positive/BioReSolvePositive.pl\"]')\n",
    "                        result = prolog_thread.query('main_do(negslice,EKs,ListReactNumbR,ListEpos,ListEneg,ListCS).')\n",
    "                        tmp_pos_set = set(chain.from_iterable(result[0]['ListEpos'])) \n",
    "                        union_pos_set.update(tmp_pos_set)\n",
    "                        tmp_neg_set = set(chain.from_iterable(result[0]['ListEneg'])) \n",
    "                        union_neg_set.update(tmp_neg_set)\n",
    "                        if (first_time):\n",
    "                            intersection_pos_set = tmp_pos_set\n",
    "                            intersection_neg_set = tmp_neg_set\n",
    "                            first_time = False\n",
    "                        else:\n",
    "                            intersection_pos_set = intersection_pos_set.intersection(tmp_pos_set)\n",
    "                            intersection_neg_set = intersection_neg_set.intersection(tmp_neg_set)\n",
    "        for f in glob.glob(\"tmp-slice*.txt\"):\n",
    "            os.remove(f)\n",
    "        for f in glob.glob(\"tmp-legenda*.txt\"):\n",
    "            os.remove(f)\n",
    "        for f in glob.glob(\"tmp-slicingrun*.txt\"):\n",
    "            os.remove(f)\n",
    "        \n",
    "        print()\n",
    "        union_pos_set=sorted(union_pos_set)\n",
    "        intersection_pos_set=sorted(intersection_pos_set)\n",
    "        union_neg_set=sorted(union_neg_set)\n",
    "        intersection_neg_set=sorted(intersection_neg_set)\n",
    "        print(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\")\n",
    "        outfile.write(\"SET OF ENTITIES IN SLICED COMPUTATIONS FOR TARGET \" + str(target) + \":\\n\")\n",
    "        print(\"UNION POSITIVE: \" + str(union_pos_set))\n",
    "        outfile.write(\"         UNION POSITIVE: \" + str(union_pos_set)+\"\\n\")\n",
    "        print(\"INTERSECTION POSITIVE: \" + str(intersection_pos_set))\n",
    "        outfile.write(\"         INTERSECTION POSITIVE: \" + str(intersection_pos_set)+\"\\n\\n\")\n",
    "        print(\"UNION NEGATIVE: \" + str(union_neg_set))\n",
    "        outfile.write(\"         UNION NEGATIVE: \" + str(union_neg_set)+\"\\n\")\n",
    "        print(\"INTERSECTION NEGATIVE: \" + str(intersection_neg_set))\n",
    "        outfile.write(\"         INTERSECTION NEGATIVE: \" + str(intersection_neg_set)+\"\\n\\n\")\n",
    "        print()\n",
    "        outfile.flush()\n",
    "    \n",
    "        generate_table(str(target.present),intersection_pos_set, union_pos_set, intersection_neg_set, union_neg_set)\n",
    "\n",
    "    outfile.close()\n",
    "    save_table_to_file(\"table_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attractors Analysis\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final.dot\"))\n",
    "print(len(G.nodes))\n",
    "print(len(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    contexts, contexts_dict = target_computations(target)\n",
    "    if (len(contexts)>0):\n",
    "        df = pd.DataFrame(contexts).sort_values(by=[8,7,6,5,4,3,2,1,0],ignore_index=True)\n",
    "        print(\"CONTEXTS THAT LEAD TO THE TARGET:\")\n",
    "        print(df)\n",
    "        print()\n",
    "        df.to_csv(\"contexts_to_\" + str(target.present) + \".csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Analysis -- Original T Cell Model -- Not Minimized\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_df = pd.DataFrame()\n",
    "slicing_analysis(\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Analysis -- Original T Cell Model -- Minimized\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec_Min.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final_Min.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_df = pd.DataFrame()\n",
    "slicing_analysis(\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Analysis -- Positive RS T Cell Model -- Not Minimized\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec_pos_nonMin.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final_pos_nonMin.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_df = pd.DataFrame()\n",
    "slicing_analysis(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Analysis -- Positive RS T Cell Model -- Minimized\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec_pos_Min.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final_pos_Min.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_df = pd.DataFrame()\n",
    "slicing_analysis(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Negative Slicing Analysis -- Original T Cell Model -- Not Minimized\n",
    "\n",
    "Before starting the analysis, uncomment directive\n",
    "\n",
    ":- [\"spec.pl\"].\n",
    "\n",
    "in BioReSolvePositive.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(nx.nx_pydot.read_dot(\"graph_complete_final.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data_df = pd.DataFrame()\n",
    "dynamic_negative_slicing_analysis(\"original\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
